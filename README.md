# 512-Data-Mining-Project
Transfer Learning

## Enviorment 
Dependencies
```bash
conda env create -f environment.yaml
```

Note if you update the enviorment be sure to update the enviroment.yaml
conda env export > environment.yaml


## Data
Using ir_datasetsc



## Data Mining Component
Our system takes the orcas dataset and creates datasets that represent query similairty in various ways. 
First, we load the queries and form hashmaps with qid-documents clicks and document-queries clicks.
We then remove all queries that do not match our thresholds(min length 5 and max length 20). 
Using this click graph we explore 3 ways of creating datasets. The target of the dataset follows the QQP dataset and is query1\tquery2\tIs_similair(binary). We are likley able to prodce an extremly large dataset which we believe will be useful in model pretraining. All of our datasets build on the notion of negative via random selection meaning for our negative signal we choose a query at random since it is fairly unlikely that random queries are closely related.
The first dataset is based on a notion of document click clustering. 

First, we load all the msmarco and orcas queries for a total of 11,416,257 queries. We then load the orcas dataset and find 18823602 query-document clicks. We then remove all queries with length < 5 or > 20 to make query distribution similair(TODO explore average query length from QQP). After query filtering we are left with 1,399,267 queries. Using this filtered data we use the following 3 methods to create data.

Graph Construction Done. There are 1399267 nodes and 18186266 edges
### Document Click clustering
For a given document, if two separate queries clicked on this document then they are considered similair. 
### Query Hopping
We turn our queries into graph nodes and for all queries there is an edge if Q1 and Qn both click on document D. This graph has 1,399,267 nodes and 18,186,266 edges. To sample the dataset we choose queries that are 1 hop away from each other where a hop is (Query -> Document -> Query). We select no more than 10 positive and negative pairs from each source edge and we select 100,000 source nodes randomly. 
### Document Hopping
We turn our data into a document graph where all documents are edges and there is an edge between D1 and D2 if there is a query that clicked on D1 and D2. This graph is more sparse(dude to the data) and it has 1,422,029 nodes and 711,875 edges. We generate out data by choosing documents that are 1 hop away from eachother (Document -> Query -> Document). Similair to the query method we select all possible pairs of similair queries and then select 10 at random. Since we also target a dataset size of 1,000,000 we sample 100,000 document nodes at random. 


### Prep data
Download QQP and then do 80:10:10 since the full dataset is there
```bash
{
  head -n 323441 > quora_training.tsv
  head -n 40430 > quora_validation.tsv
  cat > quora_eval.tsv
} < quora_duplicate_questions.tsv
{
  head -n 800000 > doc_cluster_training.tsv
  head -n 100000  > doc_cluster_validation.tsv
  cat > doc_cluster_eval.tsv
} < doc_cluster_labels.tsv
{
  head -n 800000 > query_graph_training.tsv
  head -n 100000  > query_graph_validation.tsv
  cat > query_graph_eval.tsv
} < query_graph.tsv
{
  head -n 800000 > document_graph_training.tsv
  head -n 100000 > document_graph_validation.tsv
  cat > document_graph_eval.tsv
} < document_graph.tsv
```

## Quora Question Pair Duplication Task
### Install Transformers
Install the HuggingFace interface for transformers. 
https://github.com/huggingface/transformers

### Prepare Data
Create a folder that contains 2 files - train.csv and test.csv. 
The format of the files is similar to that of the Quora Question Pairs Dataset. They should have 4 columns - idx, label, sentence1 and sentence2. Where sentence1 and sentence2 contains the two sentences to be compared, idx is an integer index of the sentence pair and column label is a binary column with a value of 0 indicating that the 2 sentences are not similar and 1 indicating that they are similar.

### Run GLUE
Finally after the data preparation, the BERT model can be trained and tested using the GLUE task of HuggingFace. The command for this is:

```bash
.\scripts\run_glue_commands.sh
```

The parameters are given in the file and are self-explantory.